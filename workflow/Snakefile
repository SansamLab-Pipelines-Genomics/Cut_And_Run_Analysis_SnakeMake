configfile: "config/config.yml"

##################################################################
##                    Define input functions                    ##
##################################################################

# this was created after reading 
#  https://eriqande.github.io/eca-bioinf-handbook/managing-workflows-with-snakemake.html
#  https://www.biostars.org/p/335903/

# this imports the pandas package functionality in an object named pd
import pandas as pd

# this reads the CSV file and sets an index using the values in the "sample" column.
samples_table = pd.read_csv(config["samples_csv"]).set_index("sample", drop=False)
samples_table = samples_table.applymap(str)

# fastq filename input function definition set to Python dictionary
def fq_dict_from_sample(wildcards):
  return {
    "fq1": samples_table.loc[wildcards.sample, "fastq1"],
    "fq2": samples_table.loc[wildcards.sample, "fastq2"]
  }

# this makes a new sample table with only the 'treatment' sample rows
treatments_table = samples_table.loc[samples_table['sampleType'] == 'treatment']

# sample_type input function definition set to Python dictionary
def sample_type_dict_from_sample(wildcards):
  return {
    "treatment": 'results/aligned_speciesOfInterest/' + treatments_table.loc[wildcards.sample, "sample"] + '.bam',
    "control": 'results/aligned_speciesOfInterest/' + treatments_table.loc[wildcards.sample, "Control"] + '.bam'
  }

# this makes a sample table with samples to merge
def samples_to_merge(table=samples_table):
  samples_table3 = table[~table['merged_sample'].isna()]
  samples_table4 = samples_table3.loc[:,('sample','merged_sample')]
  samples_table4.loc[:,'sample'] = 'results/aligned/' + samples_table4.loc[:,'sample'].astype(str) + '.bam'
  grouped_by_merged = samples_table4.groupby(samples_table4.loc[:,'merged_sample'])
  samples_table5 = grouped_by_merged["sample"].agg([' '.join])
  samples_table5.loc[:,"merged_name"] = samples_table5.index
  samples_table6 = samples_table5.drop_duplicates(subset="merged_name",keep='first')
  return samples_table6

# this makes an input function that returns a dictionary with merged filenames and bams to merge
def sample_indiv_dict_from_sampleMerged(wildcards):
  return {
    "bams_to_merge": samples_to_merge().loc[wildcards.merged_name, "join"]
  }

def bams_to_merge(table=samples_table):
  samples_table.loc[:,'sample'] = 'results/aligned/' + samples_table.loc[:,'sample'].astype(str) + '.bam'
  df = samples_table[~samples_table['merged_sample'].isna()]
  return df

def keywords_to_merge(table=samples_table):
  samples_table3 = table[~table['merged_sample'].isna()]
  samples_table4 = samples_table3.drop_duplicates(subset="merged_sample",keep='first')
  lst1 = samples_table4['merged_sample'].tolist()
  lst2 = [x for x in lst1 if x != 'nan']
  return lst2

def get_bams_to_merge(smpl):
  merged_list=samples_table['merged_sample'].tolist()
  samples_list=samples_table['sample'].tolist()
  indices=[i for i,x in enumerate(merged_list) if x==smpl]
  samples_list2 = [samples_list[i] for i in indices]
  samples_list3 = ['results/aligned/' + s + '.bam' for s in samples_list2]
  samples_string = ' '.join([str(item) for item in samples_list3])
  return samples_string

bams_to_merge_dict = {}
for m in keywords_to_merge():
  bams_to_merge_dict[m] = get_bams_to_merge(m)
  



##################################################################
##                           rules                              ##
##################################################################

# to run snakemake without explicitly requesting any output files on the command line, we must request output files in the first rule. Therefore we include this otherwise useless rule here  
rule all:
    input:
        expand("results/aligned/{sample}_merged.bam", sample = keywords_to_merge()),
        expand("results/aligned/{sample}.bam", sample = samples_table.index),
        expand("results/qc/fastqc/{sample}_R1_fastqc.html", sample = samples_table.index),
        expand("results/sicer/{sample}-W" + str(config['sicer_windowSize']) + "-G" + str(config['sicer_gapSize']) + "-FDR" + str(config['sicer_fdr']) + "-island.bed", sample = samples_table2.index),
        expand("results/macs2_normalPeaks/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_peaks.narrowPeak", sample = treatments_table.index),
        expand("results/macs2_broadPeaks/{sample}_" + str(config['macs2_broad_minimum_FDR_cutoff']) + "_peaks.broadPeak", sample = treatments_table.index),
        expand("results/bigwigs_no_spikein/{sample}.bw", sample = samples_table.index),
        expand("results/bigwigs_spikein/{sample}.bw", sample = samples_table.index)

# run fastqc on fastq.gz files before trimming
rule fastqc_reads:
    input:
        unpack(fq_dict_from_sample)   # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    output:
        html1="results/qc/fastqc/{sample}_R1_fastqc.html",
        zip1="results/qc/fastqc/{sample}_R1_fastqc.zip",
        html2="results/qc/fastqc/{sample}_R2_fastqc.html",
        zip2="results/qc/fastqc/{sample}_R2_fastqc.zip"
    conda:
        "envs/CutAndRun_Conda_Environment.yml"
    log: "results/logs/snakelogs/fastqc_reads.{sample}.log"
    shell:
        """
        fastqc {input.fq1}
        fastqc {input.fq2}
        dir=$(dirname {input.fq1})
        bsename=$(basename {input.fq1} .gz)
        bsename=$(basename ${{bsename}} .fastq)
        mv ${{dir}}/${{bsename}}_fastqc.html {output.html1}
        mv ${{dir}}/${{bsename}}_fastqc.zip {output.zip1}
        bsename=$(basename {input.fq2} .gz)
        bsename=$(basename ${{bsename}} .fastq)
        mv ${{dir}}/${{bsename}}_fastqc.html {output.html2}
        mv ${{dir}}/${{bsename}}_fastqc.zip {output.zip2}
        """

# trim reads with trimmomatic
rule trim_reads_with_trimmomatic:
    input:
        unpack(fq_dict_from_sample)   # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    output:
        trimmed1="results/trimmed/{sample}_trimmomatic_R1.fastq.gz",
        trimmed2="results/trimmed/{sample}_trimmomatic_R2.fastq.gz",
        orphaned1="results/trimmed/{sample}_trimmomatic_orphaned_R1.fastq.gz",
        orphaned2="results/trimmed/{sample}_trimmomatic_orphaned_R2.fastq.gz"
    params:
        trimmomatic_threads=config["trimmomatic_threads"],
        trimmomatic_adapterfile=config["trimmomatic_adapterfile"]
    conda:
        "envs/trim_reads_with_trimmomatic.yml"
    log: "results/logs/snakelogs/trim_reads_with_trimmomatic.{sample}.log"
    shell:
        """
        trimmomatic PE -threads {params.trimmomatic_threads} {input.fq1} {input.fq2} {output.trimmed1} {output.orphaned1} {output.trimmed2} {output.orphaned2} ILLUMINACLIP:{params.trimmomatic_adapterfile}:2:15:4:4:true LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:25
        """

# trim reads with cutadapt
rule trim_reads_with_cutadapt:
    input:
        R1="results/trimmed/{sample}_trimmomatic_R1.fastq.gz",
        R2="results/trimmed/{sample}_trimmomatic_R2.fastq.gz"
    output:
        trimmed1="results/trimmed/{sample}_trimmed_R1.fastq.gz",
        trimmed2="results/trimmed/{sample}_trimmed_R2.fastq.gz"
    params:
        cutadapt_adapterfile=config["cutadapt_adapterfile"]
    conda:
        "envs/trim_reads_with_cutadapt.yml"
    log: "results/logs/snakelogs/trim_reads_with_cutadapt.{sample}.log"
    shell:
        """
        cutadapt --cores=0 -a file:{params.cutadapt_adapterfile} -A file:{params.cutadapt_adapterfile} -a G{{100}} -A G{{100}} --minimum-length 25 --quality-cutoff 20,20 -e 0.2 --output {output.trimmed1} --paired-output {output.trimmed2} {input.R1} {input.R2}
        """

# align reads to genome
rule align_reads_with_bowtie2:
    input:
        R1="results/trimmed/{sample}_trimmed_R1.fastq.gz",
        R2="results/trimmed/{sample}_trimmed_R2.fastq.gz"
    params:
        bowtie2_genome=config["bowtie2_genome"],
        bowtie2_threads=config["bowtie2_threads"],
        bowtie2_samtools_threads=config["bowtie2_samtools_threads"]
    output:
        bam="results/aligned/{sample}.bam",
        bai="results/aligned/{sample}.bam.bai",
        completion_marker=touch("results/aligned/align_reads_with_bowtie2_{sample}.done"),
        #bam="results/aligned/{sample,.*(?<!_merged)$}.bam",
        #bai="results/aligned/{sample,.*(?<!_merged)$}.bam.bai"
    conda:
        "envs/align_reads_with_bowtie2.yml"
    log: "results/logs/snakelogs/align_reads_with_bowtie2.{sample}.log"
    shell:
        """
        bowtie2 \
        --threads {params.bowtie2_threads} \
        --dovetail \
        --phred33 \
        --maxins 2000 \
        -x {params.bowtie2_genome} \
        -1 {input.R1} \
        -2 {input.R2} \
        | \
        samtools view -b - \
        | \
        samtools sort --threads {params.bowtie2_samtools_threads} - -o {output.bam}
        # Index BAM file
        samtools index {output.bam}
        """

# merge replicates
rule merge_replicates:
    input:
        expand("results/aligned/align_reads_with_bowtie2_{sample}.done", sample = samples_table.index),
    output:
        bam="results/aligned/{sample}_merged.bam",
        bai="results/aligned/{sample}_merged.bam.bai"
    params:
        bowtie2_samtools_threads=config["bowtie2_samtools_threads"],
        inputs=lambda wildcards: bams_to_merge_dict[wildcards.sample]
        #inputs=bams_to_merge_dict["{sample}"],
    conda:
        "envs/samtools.yml"
    log: "results/logs/snakelogs/merge_replicates.{sample}.log"
    shell:
        """
        samtools merge -o {output.bam} --threads {params.bowtie2_samtools_threads} {params.inputs}
        # Index BAM file
        samtools index {output.bam}
        """
        
# get species of interest reads

rule get_species_of_interest_reads:
    input:
        bam="results/aligned/{sample}.bam"
    output:
        bam="results/aligned_speciesOfInterest/{sample}.bam"
    params:
        chromosomeSizes="resources/chromosome_sizes/" + str(config["genome_of_interest"]) + ".chrom.sizes",
        bowtie2_samtools_threads=config["bowtie2_samtools_threads"]
    conda:
        "envs/samtools.yml"
    log: "results/logs/snakelogs/get_species_of_interest_reads.{sample}.log"
    shell:
        """
        awk -F '\t' '{{ print $1,"1",$2 }}' OFS='\t' {params.chromosomeSizes} > {output.bam}tmp.bed
        samtools view --threads {params.bowtie2_samtools_threads} -bS -ML {output.bam}tmp.bed {input.bam} > {output.bam}
        samtools index {output.bam}
        rm {output.bam}tmp.bed
        """
        
# call peaks with sicer

rule call_peaks_with_sicer:
    input:
        unpack(sample_type_dict_from_sample)   # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    output:
        "results/sicer/{sample}-W" + str(config['sicer_windowSize']) + "-G" + str(config['sicer_gapSize']) + "-FDR" + str(config['sicer_fdr']) + "-island.bed"
    params:
        sicer_genome=config["sicer_genome"],
        sicer_windowSize=config["sicer_windowSize"],
        sicer_fragmentSize=config["sicer_fragmentSize"],
        sicer_fdr=config["sicer_fdr"],
        sicer_gapSize=config["sicer_gapSize"]
    conda:
        "envs/sicer2.yml"
    log: "results/logs/snakelogs/call_peaks_with_sicer.{sample}-W" + str(config['sicer_windowSize']) + "-G" + str(config['sicer_gapSize']) + "-FDR" + str(config['sicer_fdr']) + ".log"
    shell:
        """
        sicer -t {input.treatment} \
            -c {input.control} \
            -s {params.sicer_genome} \
            -w {params.sicer_windowSize} \
            -f {params.sicer_fragmentSize} \
            -fdr {params.sicer_fdr} \
            -o results/sicer/ \
            -g {params.sicer_gapSize} \
            -cpu 12
        """

    
# call narrow peaks with macs2

rule call_narrow_peaks_with_macs2:
    input:
        unpack(sample_type_dict_from_sample)   # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    output:
        "results/macs2_normalPeaks/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_peaks.narrowPeak"
    params:
        effective_genome_size=config["effective_genome_size"],
        minimum_FDR_cutoff=str(config["macs2_minimum_FDR_cutoff"]),
        sample_name="{sample}"
    conda:
        "envs/callPeaks.yml"
    log: "results/logs/snakelogs/call_narrow_peaks_with_macs2.{sample}_q" + str(config["macs2_minimum_FDR_cutoff"]) + ".log"
    shell:
        """
        macs2 callpeak \
            -t {input.treatment} \
            -c {input.control} \
            -f BAMPE \
            -g {params.effective_genome_size} \
            -n {params.sample_name}_{params.minimum_FDR_cutoff} \
            -q {params.minimum_FDR_cutoff} \
            --outdir results/macs2_normalPeaks/
        """

# call broad peaks with macs2

rule call_broad_peaks_with_macs2:
    input:
        unpack(sample_type_dict_from_sample)   # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    output:
        "results/macs2_broadPeaks/{sample}_" + str(config['macs2_broad_minimum_FDR_cutoff']) + "_peaks.broadPeak"
    params:
        effective_genome_size=config["effective_genome_size"],
        minimum_FDR_cutoff=config["macs2_broad_minimum_FDR_cutoff"],
        sample_name="{sample}"
    conda:
        "envs/callPeaks.yml"
    log: "results/logs/snakelogs/call_broad_peaks_with_macs2.{sample}_q" + str(config["macs2_broad_minimum_FDR_cutoff"]) + ".log"
    shell:
        """
        macs2 callpeak \
            -t {input.treatment} \
            -c {input.control} \
            -f BAMPE \
            -g {params.effective_genome_size} \
            -n {params.sample_name}_{params.minimum_FDR_cutoff} \
            -q {params.minimum_FDR_cutoff} \
            --broad \
            --outdir results/macs2_broadPeaks/
        """


# make bigwigs without spike-in normalization
  
rule make_bigwigs_no_spikein:
    input:
      bam="results/aligned_speciesOfInterest/{sample}.bam"
    output:
      bw="results/bigwigs_no_spikein/{sample}.bw"
    params:
      gs=config["effective_genome_size"],
      bs=config["bamCoverage_binSize"],
      bl=config["blacklist_file"]
    conda:
        "envs/deeptools.yml"
    log: "results/logs/snakelogs/make_bigwigs_no_spikein_{sample}.log"
    shell:
      """
      bamCoverage -b {input.bam} -o {output.bw} --effectiveGenomeSize {params.gs} --binSize {params.bs} --numberOfProcessors max --verbose --blackListFileName {params.bl} --centerReads
      """

rule calculate_spikein_scale_factors:
    input:
      bam="results/aligned/{sample}.bam"
    output:
      "results/scale_factors/{sample}_scaleFactor.txt"
    params:
      px=config["spike_in_chrom_prefix"]
    conda:
        "envs/samtools.yml"
    log: "results/logs/snakelogs/calculate_spikein_scale_factors_{sample}.log"
    shell:
      """
      total_counts=$(samtools idxstats {input.bam} | awk -F'\t' '{{sum+=$3;}} END{{print sum;}}')
      spike_in_counts=$(samtools idxstats {input.bam} | grep {params.px} | awk -F'\t' '{{sum+=$3;}} END{{print sum;}}')
      spike_in_percentage=$(echo "scale=3; $spike_in_counts/$total_counts*100" | bc )
      scale_factor=$(echo "scale=8; 1000000/$spike_in_counts" | bc )
      echo "spike in read counts = ${{spike_in_counts}}" > {output}_tmp
      echo "spike in percentage = ${{spike_in_percentage}}" >> {output}_tmp
      echo "scale factor = ${{scale_factor}}" >> {output}_tmp
      printf "%8.4f\\n" "${{scale_factor}}" >> {output}_tmp
      mv {output}_tmp {output}
      """

rule make_bigwigs_with_spikein:
    input:
      scale="results/scale_factors/{sample}_scaleFactor.txt",
      bam="results/aligned_speciesOfInterest/{sample}.bam"
    output:
      bw="results/bigwigs_spikein/{sample}.bw"
    params:
      gs=config["effective_genome_size"],
      bs=config["bamCoverage_binSize"],
      bl=config["blacklist_file"],
      px=config["spike_in_chrom_prefix"]
    conda:
        "envs/deeptools.yml"
    log: "results/logs/snakelogs/make_bigwigs_with_spikein_{sample}.log"
    shell:
      """
      scale_factor=$(grep scale {input.scale} | sed 's/scale factor = //g')
      bamCoverage -b {input.bam} -o {output.bw} --effectiveGenomeSize {params.gs} --binSize {params.bs} --numberOfProcessors max --verbose --blackListFileName {params.bl} --centerReads --scaleFactor ${{scale_factor}}
      """
