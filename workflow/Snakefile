configfile: "config/config.yml"


##################################################################
##                       import modules                         ##
##################################################################

import pandas as pd
import re
import common_functions as cf

##################################################################
##                read and modify samples table                 ##
##################################################################

# this reads the CSV file and sets an index using the values in the sample column.
samples_table = pd.read_csv(config["samples_csv"]).set_index("sample", drop=False)
samples_table = samples_table.applymap(str)
samples_table_w_merged_suffix = cf.add_merge_suffix_to_merged_samples(samples_table)


##################################################################
##                           rules                              ##
##################################################################


# to run snakemake without explicitly requesting any output files on the command line, we must request output files in the first rule. Therefore we include this otherwise useless rule here
rule all:
    input:
        expand(
            f"results/sicer/{{sample}}-W{str(config['sicer_windowSize'])}-G{str(config['sicer_gapSize'])}-FDR{str(config['sicer_fdr'])}-island.bed",
            sample=cf.make_all_treatments_table(samples_table_w_merged_suffix)[
                "sample"
            ].to_list(),
        ),
        expand(
            f"results/macs2_normalPeaks/{{sample}}_{str(config['macs2_minimum_FDR_cutoff'])}_peaks.narrowPeak",
            sample=cf.make_all_treatments_table(samples_table_w_merged_suffix)[
                "sample"
            ].to_list(),
        ),
        expand(
            f"results/macs2_broadPeaks/{{sample}}_{str(config['macs2_broad_minimum_FDR_cutoff'])}_peaks.broadPeak",
            sample=cf.make_all_treatments_table(samples_table_w_merged_suffix)[
                "sample"
            ].to_list(),
        ),
        expand(
            "results/bigwigs_no_spikein/{sample}.bw",
            sample=cf.keywords_to_merge(samples_table_w_merged_suffix)
            + samples_table["sample"].to_list(),
        ),
        expand(
            f"results/bigwigs_spikein/{{sample}}_{str(config['spike_in_chrom_prefix'])}.bw",
            sample=cf.keywords_to_merge(samples_table_w_merged_suffix)
            + samples_table["sample"].to_list(),
        ),
        expand(
            "results/read_stats/{sample}.csv",
            sample=samples_table["sample"].to_list()
        ),
        expand(
            f"results/fingerprint_plots/{{sample}}_fingerprint.png",
            sample=cf.make_all_treatments_table(samples_table_w_merged_suffix)[
                "sample"
            ].to_list()
        ),
        expand(
            f"results/fingerprint_quality_metrics/{{sample}}_fingerprint_quality_metrics.txt",
            sample=cf.make_all_treatments_table(samples_table_w_merged_suffix)[
                "sample"
            ].to_list()
        ),
        expand(
            f"results/fragment_sizes_plots/{{sample}}_fragment_sizes.png",
            sample=cf.make_all_treatments_table(samples_table_w_merged_suffix)[
                "sample"
            ].to_list()
        ),
        

        
rule trim_reads_with_trimmomatic:
    input:
        unpack(
            lambda wildcards: {
                "fq1": samples_table.loc[wildcards.sample, "fastq1"],
                "fq2": samples_table.loc[wildcards.sample, "fastq2"],
            }
        ),  # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    output:
        trimmed1=temp("results/trimmed/{sample}_trimmomatic_R1.fastq.gz"),
        trimmed2=temp("results/trimmed/{sample}_trimmomatic_R2.fastq.gz"),
        orphaned1=temp("results/trimmed/{sample}_trimmomatic_orphaned_R1.fastq.gz"),
        orphaned2=temp("results/trimmed/{sample}_trimmomatic_orphaned_R2.fastq.gz"),
    params:
        trimmomatic_threads=config["trimmomatic_threads"],
        trimmomatic_adapterfile=config["trimmomatic_adapterfile"],
    conda:
        "envs/trim_reads_with_trimmomatic.yml"
    log:
        "results/logs/snakelogs/trim_reads_with_trimmomatic.{sample}.log",
    wildcard_constraints:
        sample="((?!_merged).)*",
    shell:
        """
        trimmomatic PE -threads {params.trimmomatic_threads} -trimlog {log} {input.fq1} {input.fq2} {output.trimmed1} {output.orphaned1} {output.trimmed2} {output.orphaned2} ILLUMINACLIP:{params.trimmomatic_adapterfile}:2:15:4:4:true LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:25
        """


rule trim_reads_with_cutadapt:
    input:
        R1="results/trimmed/{sample}_trimmomatic_R1.fastq.gz",
        R2="results/trimmed/{sample}_trimmomatic_R2.fastq.gz",
    output:
        trimmed1=temp("results/trimmed/{sample}_trimmed_R1.fastq.gz"),
        trimmed2=temp("results/trimmed/{sample}_trimmed_R2.fastq.gz"),
    params:
        cutadapt_adapterfile=config["cutadapt_adapterfile"],
    conda:
        "envs/trim_reads_with_cutadapt.yml"
    log:
        "results/logs/snakelogs/trim_reads_with_cutadapt.{sample}.log",
    wildcard_constraints:
        sample="((?!_merged).)*",
    shell:
        """
        cutadapt --json {log} --cores=0 -a file:{params.cutadapt_adapterfile} -A file:{params.cutadapt_adapterfile} -a G{{100}} -A G{{100}} --minimum-length 25 --quality-cutoff 20,20 -e 0.2 --output {output.trimmed1} --paired-output {output.trimmed2} {input.R1} {input.R2}
        """


rule align_reads_with_bowtie2:
    input:
        R1="results/trimmed/{sample}_trimmed_R1.fastq.gz",
        R2="results/trimmed/{sample}_trimmed_R2.fastq.gz",
    params:
        bowtie2_genome=config["bowtie2_genome"],
        bowtie2_threads=config["bowtie2_threads"],
        bowtie2_samtools_threads=config["bowtie2_samtools_threads"],
    output:
        bam="results/aligned/{sample}.bam",
        bai="results/aligned/{sample}.bam.bai",
    wildcard_constraints:
        sample="((?!_merged).)*",
    conda:
        "envs/align_reads_with_bowtie2.yml"
    log:
        "results/logs/snakelogs/align_reads_with_bowtie2.{sample}.log",
    shell:
        """
        bowtie2 --met-file {log} --threads {params.bowtie2_threads} --dovetail --phred33 --maxins 2000 -x {params.bowtie2_genome} -1 {input.R1} -2 {input.R2} | samtools view -b - | samtools sort --threads {params.bowtie2_samtools_threads} - -o {output.bam}
        samtools index {output.bam}
        """
        
        
rule quality_filter_aligned_reads:
    input:
        bam="results/aligned/{sample}.bam",
    output:
        bam="results/aligned_and_filtered/{sample}.bam",
        bai="results/aligned_and_filtered/{sample}.bam.bai"
    params:
        bowtie2_samtools_threads=config["bowtie2_samtools_threads"],
        flag=config["samtools_filter_flag"],
    wildcard_constraints:
        sample="((?!_merged).)*",
    conda:
        "envs/samtools.yml"
    shell:
        """
        samtools view -h --threads {params.bowtie2_samtools_threads} -b -q 10 -F {params.flag} {input.bam} > {output.bam}
        samtools index {output.bam}
        """
        

rule merge_replicates:
    input:
        expand(
            "results/aligned_and_filtered/{sample}.bam",
            sample=cf.make_samples_to_merge_list(samples_table_w_merged_suffix),
        ),
    output:
        bam="results/aligned_and_filtered/{sample}.bam",
        bai="results/aligned_and_filtered/{sample}.bam.bai",
    params:
        bowtie2_samtools_threads=config["bowtie2_samtools_threads"],
        inputs=lambda wildcards: cf.make_bams_to_merge_dict(
            cf.keywords_to_merge(samples_table_w_merged_suffix),
            samples_table_w_merged_suffix,
        )[wildcards.sample],
    wildcard_constraints:
        sample=".*_merged.*",
    conda:
        "envs/samtools.yml"
    shell:
        """
        samtools merge -o {output.bam} --threads {params.bowtie2_samtools_threads} {params.inputs}
        samtools index {output.bam}
        """


rule get_species_of_interest_reads:
    input:
        bam="results/aligned_and_filtered/{sample}.bam",
    output:
        bam="results/aligned_speciesOfInterest/{sample}.bam",
    params:
        chromosomeSizes=f"resources/chromosome_sizes/{str(config['genome_of_interest'])}.chrom.sizes",
        bowtie2_samtools_threads=config["bowtie2_samtools_threads"],
    conda:
        "envs/samtools.yml"
    log:
        "results/logs/snakelogs/get_species_of_interest_reads.{sample}.log",
    shell:
        """
        awk -F '\t' '{{ print $1,"1",$2 }}' OFS='\t' {params.chromosomeSizes} > {output.bam}tmp.bed
        samtools view --threads {params.bowtie2_samtools_threads} -bS -ML {output.bam}tmp.bed {input.bam} > {output.bam}
        samtools index {output.bam}
        echo "Number of species-of-interest reads:" > {log}
        samtools view --threads {params.bowtie2_samtools_threads} -c {output.bam} >> {log}
        rm {output.bam}tmp.bed
        """


rule call_peaks_with_sicer:
    input:
        unpack(
            lambda wildcards: {
                "treatment": f"results/aligned_speciesOfInterest/{cf.make_all_treatments_table(samples_table_w_merged_suffix).loc[wildcards.sample, 'sample']}.bam",
                "control": f"results/aligned_speciesOfInterest/{cf.make_all_treatments_table(samples_table_w_merged_suffix).loc[wildcards.sample, 'Control']}.bam",
            }
        ),  # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    output:
        f"results/sicer/{{sample}}-W{str(config['sicer_windowSize'])}-G{str(config['sicer_gapSize'])}-FDR{str(config['sicer_fdr'])}-island.bed",
    params:
        sicer_genome=config["sicer_genome"],
        sicer_windowSize=config["sicer_windowSize"],
        sicer_fragmentSize=config["sicer_fragmentSize"],
        sicer_fdr=config["sicer_fdr"],
        sicer_gapSize=config["sicer_gapSize"],
    conda:
        "envs/sicer2.yml"
    shell:
        """
        sicer -t {input.treatment} -c {input.control} -s {params.sicer_genome} -w {params.sicer_windowSize} -f {params.sicer_fragmentSize} -fdr {params.sicer_fdr} -o results/sicer/ -g {params.sicer_gapSize} -cpu 12
        """


rule call_narrow_peaks_with_macs2:
    input:
        unpack(
            lambda wildcards: {
                "treatment": f"results/aligned_speciesOfInterest/{cf.make_all_treatments_table(samples_table_w_merged_suffix).loc[wildcards.sample, 'sample']}.bam",
                "control": f"results/aligned_speciesOfInterest/{cf.make_all_treatments_table(samples_table_w_merged_suffix).loc[wildcards.sample, 'Control']}.bam",
            }
        ),  # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    output:
        f"results/macs2_normalPeaks/{{sample}}_{str(config['macs2_minimum_FDR_cutoff'])}_peaks.narrowPeak",
    params:
        effective_genome_size=config["effective_genome_size"],
        minimum_FDR_cutoff=str(config["macs2_minimum_FDR_cutoff"]),
        sample_name="{sample}",
    conda:
        "envs/callPeaks.yml"
    log:
        f"results/logs/snakelogs/call_narrow_peaks_with_macs2.{{sample}}_q{str(config['macs2_minimum_FDR_cutoff'])}.log",
    shell:
        """
        macs2 --version > {log}
        macs2 callpeak -t {input.treatment} -c {input.control} -f BAMPE -g {params.effective_genome_size} -n {params.sample_name}_{params.minimum_FDR_cutoff} -q {params.minimum_FDR_cutoff} --outdir results/macs2_normalPeaks/
        """


rule call_broad_peaks_with_macs2:
    input:
        unpack(
            lambda wildcards: {
                "treatment": f"results/aligned_speciesOfInterest/{cf.make_all_treatments_table(samples_table_w_merged_suffix).loc[wildcards.sample, 'sample']}.bam",
                "control": f"results/aligned_speciesOfInterest/{cf.make_all_treatments_table(samples_table_w_merged_suffix).loc[wildcards.sample, 'Control']}.bam",
            }
        ),  # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    output:
        f"results/macs2_broadPeaks/{{sample}}_{str(config['macs2_broad_minimum_FDR_cutoff'])}_peaks.broadPeak",
    params:
        effective_genome_size=config["effective_genome_size"],
        minimum_FDR_cutoff=config["macs2_broad_minimum_FDR_cutoff"],
        sample_name="{sample}",
    conda:
        "envs/callPeaks.yml"
    log:
        "results/logs/snakelogs/call_broad_peaks_with_macs2.{sample}_q"
        + str(config["macs2_broad_minimum_FDR_cutoff"])
        + ".log",
    shell:
        """
        macs2 --version > {log}
        macs2 callpeak -t {input.treatment} -c {input.control} -f BAMPE -g {params.effective_genome_size} -n {params.sample_name}_{params.minimum_FDR_cutoff} -q {params.minimum_FDR_cutoff} --broad --outdir results/macs2_broadPeaks/
        """

  
rule make_fragment_sizes_plots:
    input:
        unpack(
            lambda wildcards: {
                "treatment": f"results/aligned_speciesOfInterest/{cf.make_all_treatments_table(samples_table_w_merged_suffix).loc[wildcards.sample, 'sample']}.bam",
                "control": f"results/aligned_speciesOfInterest/{cf.make_all_treatments_table(samples_table_w_merged_suffix).loc[wildcards.sample, 'Control']}.bam",
            }
        ),  # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    output:
        fp=report(f"results/fragment_sizes_plots/{{sample}}_fragment_sizes.png"),
        fl=f"results/fragment_sizes/{{sample}}_fragment_sizes.txt",
        fs=f"results/fragment_sizes/{{sample}}_fragment_sizes_stats.txt",
    params:
        bl=config["blacklist_file"],
        processors=config["bowtie2_samtools_threads"],
        max_size_to_plot=1000
    conda:
        "envs/deeptools.yml"
    shell:
        """
        mkdir -p results/fragment_sizes_plots/
        mkdir -p results/fragment_sizes/
        touch {output.fl}
        bamPEFragmentSize --maxFragmentLength {params.max_size_to_plot} --outRawFragmentLengths {output.fl} --table {output.fs} --numberOfProcessors {params.processors} --blackListFileName {params.bl} -b {input.treatment} {input.control} --histogram {output.fp}
        """    
        
        
rule make_fingerprint_plots:
    input:
        unpack(
            lambda wildcards: {
                "treatment": f"results/aligned_speciesOfInterest/{cf.make_all_treatments_table(samples_table_w_merged_suffix).loc[wildcards.sample, 'sample']}.bam",
                "control": f"results/aligned_speciesOfInterest/{cf.make_all_treatments_table(samples_table_w_merged_suffix).loc[wildcards.sample, 'Control']}.bam",
            }
        ),  # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    output:
        fp=report(f"results/fingerprint_plots/{{sample}}_fingerprint.png"),
        qm=f"results/fingerprint_quality_metrics/{{sample}}_fingerprint_quality_metrics.txt",
    params:
        bl=config["blacklist_file"],
    conda:
        "envs/deeptools.yml"
    shell:
        """
        plotFingerprint --outQualityMetrics {output.qm} --JSDsample {input.control} --numberOfProcessors max --smartLabels --blackListFileName {params.bl} -b {input.treatment} {input.control} -plot {output.fp}
        """    
        
        
rule make_bigwigs_no_spikein:
    input:
        bam="results/aligned_speciesOfInterest/{sample}.bam",
    output:
        bw="results/bigwigs_no_spikein/{sample}.bw",
    params:
        gs=config["effective_genome_size"],
        bs=config["bamCoverage_binSize"],
        bl=config["blacklist_file"],
    conda:
        "envs/deeptools.yml"
    log:
        "results/logs/snakelogs/make_bigwigs_no_spikein_{sample}.log",
    shell:
        """
        bamCoverage --version > {log}
        bamCoverage -b {input.bam} -o {output.bw} --effectiveGenomeSize {params.gs} --binSize {params.bs} --numberOfProcessors max --verbose --blackListFileName {params.bl} --centerReads
        """


rule calculate_spikein_scale_factors:
    input:
        bam="results/aligned_and_filtered/{sample}.bam",
    output:
        report("results/scale_factors/{sample}_scaleFactor.txt"),
    params:
        px=config["spike_in_chrom_prefix"],
    conda:
        "envs/samtools.yml"
    shell:
        """
        total_counts=$(samtools idxstats {input.bam} | awk -F'\t' '{{sum+=$3;}} END{{print sum;}}')
        spike_in_counts=$(samtools idxstats {input.bam} | grep {params.px} | awk -F'\t' '{{sum+=$3;}} END{{print sum;}}')
        spike_in_percentage=$(echo "scale=3; $spike_in_counts/$total_counts*100" | bc )
        scale_factor=$(echo "scale=8; 1000000/$spike_in_counts" | bc )
        echo "bam file:  {input.bam}" > {output}_tmp
        echo "total counts = ${{total_counts}}" >> {output}_tmp
        echo "spike in read counts = ${{spike_in_counts}}" >> {output}_tmp
        echo "spike in percentage = ${{spike_in_percentage}}" >> {output}_tmp
        printf "scale factor = ""%-20.8f\\n" "${{scale_factor}}" >> {output}_tmp
        mv {output}_tmp {output}
        """


rule make_bigwigs_with_spikein:
    input:
        scale="results/scale_factors/{sample}_scaleFactor.txt",
        bam="results/aligned_speciesOfInterest/{sample}.bam",
    output:
        bw=f"results/bigwigs_spikein/{{sample}}_{str(config['spike_in_chrom_prefix'])}.bw"
    params:
        gs=config["effective_genome_size"],
        bs=config["bamCoverage_binSize"],
        bl=config["blacklist_file"],
        px=config["spike_in_chrom_prefix"],
    conda:
        "envs/deeptools.yml"
    log:
        "results/logs/snakelogs/make_bigwigs_with_spikein_{sample}.log",
    shell:
        """
        bamCoverage --version > {log}
        scale_factor=$(grep scale {input.scale} | sed 's/scale factor = //g')
        bamCoverage -b {input.bam} -o {output.bw} --effectiveGenomeSize {params.gs} --binSize {params.bs} --numberOfProcessors max --verbose --blackListFileName {params.bl} --centerReads --scaleFactor ${{scale_factor}}
        """

rule calculate_alignment_and_filtering_counts:
    input:
        aligned_bam="results/aligned/{sample}.bam",
        filtered_bam="results/aligned_and_filtered/{sample}.bam",
        species_bam="results/aligned_speciesOfInterest/{sample}.bam",
    output:
        "results/read_stats/{sample}.csv"
    params:
        goi=config["genome_of_interest"]
    wildcard_constraints:
        sample="((?!_merged).)*",
    conda:
        "envs/samtools.yml"
    log:
    shell:
        """
        TOTAL_READS=$(samtools view -c {input.aligned_bam})
        FILTERED_READS=$(samtools view -c {input.filtered_bam})
        SPECIES_READS=$(samtools view -c {input.species_bam})
        echo "total read count, ${{TOTAL_READS}}" > {output}_tmp
        echo "filtered read count, ${{FILTERED_READS}}" >> {output}_tmp
        echo "{params.goi} read count, ${{FILTERED_READS}}" >> {output}_tmp
        mv {output}_tmp {output}
        """

               
 rule make_html_report:
        input:
            expand("results/read_stats/{sample}.csv",sample=samples_table["sample"].to_list()),
            expand(f"results/fingerprint_plots/{{sample}}_fingerprint.png",sample=cf.make_all_treatments_table(samples_table_w_merged_suffix)["sample"].to_list()),
            expand(f"results/fingerprint_quality_metrics/{{sample}}_fingerprint_quality_metrics.txt",sample=cf.make_all_treatments_table(samples_table_w_merged_suffix)["sample"].to_list()),
            expand(f"results/fragment_sizes_plots/{{sample}}_fragment_sizes.png",sample=cf.make_all_treatments_table(samples_table_w_merged_suffix)["sample"].to_list()),
        output:
            "results/Report.html"
        params:
            samples=cf.make_all_treatments_table(samples_table_w_merged_suffix)["sample"].to_list(),
        envmodules:
            config["R"],
            config["Bioconductor"]
        conda:
            "envs/R_Report.yml"
        log: "results/logs/snakelogs/{sample}_merge_count_tables.log"
        script:
            "scripts/Make_final_report.R"
