configfile: "config/config.yml"


##################################################################
##                       import modules                         ##
##################################################################

# Import required libraries
import pandas as pd
import re
import common_functions as cf
import shutil
from datetime import datetime

# Get the current date and time
now = datetime.now()

# Create a timestamp string in the format "YYMMDD_HHmm" using the current date and time
tmstmp = str(now.strftime("%y%m%d_%H%M"))


##################################################################
##                Read and modify samples table                 ##
##################################################################

# Read the CSV file specified in the config and set the index using the values in the 'sample' column
samples_table = pd.read_csv(config["samples_csv"]).set_index("sample", drop=False)

# Drop rows with NaN values in the 'Control' column, then set the index using the values in the 'merged_sample' column
samples_table_noInputs = samples_table.dropna(subset=["Control"]).set_index("merged_sample", drop=False)

# Get a list of unique 'merged_sample' values from the samples_table
merged_samples = list(set(samples_table["merged_sample"]))

# Convert each item in merged_samples to a string
merged_samples = [str(i) for i in merged_samples]

# Get a list of unique 'merged_sample' values from the samples_table_noInputs
merged_samples_noInputs = list(set(samples_table_noInputs["merged_sample"]))

# Convert each item in merged_samples_noInputs to a string
merged_samples_noInputs = [str(i) for i in merged_samples_noInputs]

# Get a list of unique 'sample' values from the samples_table
all_samples = list(set(samples_table["sample"]))

# Convert each item in all_samples to a string
all_samples = [str(i) for i in all_samples]

samples_table_w_merged_suffix = cf.add_merge_suffix_to_merged_samples(samples_table)


##################################################################
##        set all ulitimate output files in "rule all"          ##
##################################################################

# Rule 'all' serves as a global entry point, specifying output files that must be generated
rule all:
    input:
        # Expand rule generates multiple input/output file names based on the provided lists
        # Aligned bam files for each merged sample
        expand("results/aligned_speciesOfInterest/{sample}.bam", sample=merged_samples),
        # SICER output files
        expand(
            f"results/sicer/{{sample}}-W{str(config['sicer_windowSize'])}-G{str(config['sicer_gapSize'])}-FDR{str(config['sicer_fdr'])}-island.bed",
            sample=merged_samples_noInputs
        ) if config['sicer_run'] == True else ".placeholder",
        # MACS2 normal peak output files
        expand(
            f"results/macs2_normalPeaks/{{sample}}_{str(config['macs2_minimum_FDR_cutoff'])}_peaks.narrowPeak",
            sample=merged_samples_noInputs
        ) if config['macs2_run'] == True else ".placeholder",
        # MACS2 broad peak output files
        expand(
            f"results/macs2_broadPeaks/{{sample}}_{str(config['macs2_broad_minimum_FDR_cutoff'])}_peaks.broadPeak",
            sample=merged_samples_noInputs
        ) if config['macs2Broad_run'] == True else ".placeholder",
        # Merged bam files
        expand("results/merged/{sample}.bam", sample=merged_samples),
        # BigWig files with spike-in
        expand(f"results/bigwigs_spikein/{{sample}}_{str(config['spike_in_chrom_prefix'])}.bw", sample=merged_samples_noInputs),
        # BigWig files without spike-in
        expand("results/bigwigs_no_spikein/{sample}.bw", sample=merged_samples_noInputs),
        # Background normalized BigWig files
        expand(f"results/backgroundNormalizedBigwigs/{{sample}}_bkgrndNorm.bw", sample=merged_samples_noInputs)

##################################################################
##                         trim reads                           ##
##################################################################

# Rule 'trim_reads_with_trimmomatic' generates fastqc reports and then trims the raw reads using Trimmomatic.
rule trim_reads_with_trimmomatic:
    input:
        # Input raw fastq files from the samples table
        unpack(
            lambda wildcards: {
                "fq1": samples_table.loc[wildcards.sample, "fastq1"],
                "fq2": samples_table.loc[wildcards.sample, "fastq2"],
            }
        ),  # Unpack() function turns the dict into a collection of named inputs
    output:
        # Output trimmed fastq files and orphaned reads
        trimmed1=temp("results/trimmed/{sample}_trimmomatic_R1.fastq.gz"),
        trimmed2=temp("results/trimmed/{sample}_trimmomatic_R2.fastq.gz"),
        orphaned1=temp("results/trimmed/{sample}_trimmomatic_orphaned_R1.fastq.gz"),
        orphaned2=temp("results/trimmed/{sample}_trimmomatic_orphaned_R2.fastq.gz"),
    params:
        # Trimmomatic parameters from the config file
        trimmomatic_threads=config["trimmomatic_threads"],
        trimmomatic_adapterfile=config["trimmomatic_adapterfile"],
    conda:
        # Conda environment for Trimmomatic
        "envs/trim_reads_with_trimmomatic.yml"
    envmodules:
        # Trimmomatic environment module from the config file
        config["trimmomatic"],
        config["fastqc"]
    log:
        # Log file for trim_reads_with_trimmomatic rule
        "results/logs/snakelogs/trim_reads_with_trimmomatic.{sample}.log",
    shell:
        # Trimmomatic command to run on the input fastq files with the specified parameters
        """
        # Create the 'results/fastqc_results' directory if it doesn't exist
        mkdir -p results/fastqc_results

        # Run FastQC on input files (input.fq1 and input.fq2) with 2 threads and save the output in 'results/fastqc_results/' directory
        fastqc -t 2 -o results/fastqc_results/ {input.fq1} {input.fq2}

        # Run Trimmomatic on paired-end input files (input.fq1 and input.fq2) with specified parameters for adapter clipping, quality trimming, and minimum read length
        # Save the output trimmed and orphaned reads in the respective output files
        trimmomatic PE -threads {params.trimmomatic_threads} {input.fq1} {input.fq2} {output.trimmed1} {output.orphaned1} {output.trimmed2} {output.orphaned2} ILLUMINACLIP:{params.trimmomatic_adapterfile}:2:15:4:4:true LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:25
        """

# Rule 'trim_reads_with_cutadapt' trims the reads using Cutadapt after Trimmomatic and then generates a final fastqc report
rule trim_reads_with_cutadapt:
    input:
        # Input Trimmomatic trimmed fastq files
        R1="results/trimmed/{sample}_trimmomatic_R1.fastq.gz",
        R2="results/trimmed/{sample}_trimmomatic_R2.fastq.gz",
    output:
        # Output Cutadapt trimmed fastq files
        trimmed1=temp("results/trimmed/{sample}_trimmed_R1.fastq.gz"),
        trimmed2=temp("results/trimmed/{sample}_trimmed_R2.fastq.gz"),
    params:
        # Cutadapt adapter file from the config file
        cutadapt_adapterfile=config["cutadapt_adapterfile"],
    conda:
        # Conda environment for Cutadapt
        "envs/trim_reads_with_cutadapt.yml"
    envmodules:
        # Cutadapt environment module from the config file
        config["cutadapt"],
        config["fastqc"]
    log:
        # Log file for trim_reads_with_cutadapt rule
        "results/logs/snakelogs/trim_reads_with_cutadapt.{sample}.log",
    shell:
        # Cutadapt command to run on the input fastq files with the specified parameters
        """
        # Run Cutadapt on paired-end input files (input.R1 and input.R2) with specified parameters for adapter removal, quality trimming, and minimum read length
        # Save the output trimmed reads in the respective output files and log the results in a JSON format
        cutadapt --json {log} --cores=0 -a file:{params.cutadapt_adapterfile} -A file:{params.cutadapt_adapterfile} -a G{{100}} -A G{{100}} --minimum-length 25 --quality-cutoff 20,20 -e 0.2 --output {output.trimmed1} --paired-output {output.trimmed2} {input.R1} {input.R2}

        # Create the 'results/fastqc_results_after_trimming' directory if it doesn't exist
        mkdir -p results/fastqc_results_after_trimming

        # Run FastQC on the output trimmed files (output.trimmed1 and output.trimmed2) with 2 threads and save the output in 'results/fastqc_results_after_trimming/' directory
        fastqc -t 2 -o results/fastqc_results_after_trimming/ {output.trimmed1} {output.trimmed2}
        """

##################################################################
##                        align reads                           ##
##################################################################

# This rule defines the alignment step using bowtie2 in a Snakemake workflow
rule align_reads_with_bowtie2:
    input:
        # Input trimmed read files (R1 and R2)
        R1="results/trimmed/{sample}_trimmed_R1.fastq.gz",
        R2="results/trimmed/{sample}_trimmed_R2.fastq.gz",
    params:
        # Parameters for bowtie2 and samtools
        bowtie2_genome=config["bowtie2_genome"],
        bowtie2_threads=config["bowtie2_threads"],
        bowtie2_samtools_threads=config["bowtie2_samtools_threads"],
    output:
        # Output BAM file and its index
        bam="results/aligned/{sample}.bam",
        bai="results/aligned/{sample}.bam.bai",
    wildcard_constraints:
        # Exclude samples with '_merged' in their names
        sample="((?!_merged).)*",
    conda:
        # Conda environment with required software dependencies
        "envs/align_reads_with_bowtie2.yml"
    envmodules:
        # Load necessary environment modules
        config["bowtie2"],
        config["samtools"],
    log:
        # Log file for the alignment step
        "results/logs/snakelogs/align_reads_with_bowtie2.{sample}.log",
    shell:
        """
        # Run bowtie2 for paired-end read alignment and pipe the output to samtools for BAM conversion and sorting
        bowtie2 --met-file {log} --threads {params.bowtie2_threads} --dovetail --phred33 --maxins 2000 -x {params.bowtie2_genome} -1 {input.R1} -2 {input.R2} | samtools view -b - | samtools sort --threads {params.bowtie2_samtools_threads} - -o {output.bam}
        # Index the output BAM file
        samtools index {output.bam}
        """

##################################################################
##                    filter and merge bams                     ##
##################################################################

# This rule defines the quality filtering step in a Snakemake workflow using samtools
rule quality_filter_aligned_reads:
    input:
        # Input BAM file from the alignment step
        bam="results/aligned/{sample}.bam",
    output:
        # Output quality-filtered BAM file and its index
        bam="results/aligned_and_filtered/{sample}.bam",
        bai="results/aligned_and_filtered/{sample}.bam.bai"
    params:
        # Parameters for samtools
        bowtie2_samtools_threads=config["bowtie2_samtools_threads"],
        flag=config["samtools_filter_flag"],
    conda:
        # Conda environment with required software dependencies
        "envs/samtools.yml"
    envmodules:
        # Load necessary environment modules
        config["samtools"]
    shell:
        """
        # Filter the aligned reads based on mapping quality and flags using samtools
        samtools view -h --threads {params.bowtie2_samtools_threads} -b -q 10 -F {params.flag} {input.bam} > {output.bam}
        # Index the output filtered BAM file
        samtools index {output.bam}
        """

rule merge_replicates:
    input:
        expand(
            "results/aligned_and_filtered/{sample}.bam",
            sample=cf.make_samples_to_merge_list(samples_table_w_merged_suffix),
        ),
    params:
        bams=lambda wildcards: cf.bamFilenames_from_mergedSamples(wildcards.sample,samples_table),
        bowtie2_samtools_threads=config["bowtie2_samtools_threads"],
    output:
        bam="results/merged/{sample}.bam",
        bai="results/merged/{sample}.bam.bai",
    envmodules:
        config["samtools"],
    shell:
        """
        # Check if the input BAM file parameter does not contain any spaces
        if [[ "{params.bams}" != *" "* ]]; then
            # If there are no spaces (ie there are not multiple files to merge), copy the input BAM file to the output BAM file
            cp {params.bams} {output.bam}
        else
            # If there are spaces, merge the input BAM files using samtools
            samtools merge -@ {params.bowtie2_samtools_threads} {output.bam} {params.bams}
        fi
        """

# This rule defines the extraction of reads corresponding to the species of interest in a Snakemake workflow using samtools
rule get_species_of_interest_reads:
    input:
        # Input BAM file containing reads from multiple species
        bam="results/merged/{sample}.bam",
    output:
        # Output BAM file containing only the reads from the species of interest
        bam="results/aligned_speciesOfInterest/{sample}.bam",
    params:
        # Parameters for samtools and filtering
        bowtie2_samtools_threads=config["bowtie2_samtools_threads"],
        spike_prefix=config["spike_in_chrom_prefix"],
        ste=config["chromosome_strings_to_exclude"]
    conda:
        # Conda environment with required software dependencies
        "envs/samtools.yml"
    envmodules:
        # Load necessary environment modules
        config["samtools"]
    log:
        # Log file to store the number of species-of-interest reads
        "results/logs/snakelogs/get_species_of_interest_reads.{sample}.log",
    shell:
        """
        # Generate a list of chromosomes from the input BAM file, excluding spike-ins and unwanted chromosome strings
        LIST_STRING=$(samtools idxstats {input.bam} | cut -f1 | grep -v -P ^{params.spike_prefix} | grep -v -P ^EC | grep -v '{params.ste}' | grep -v \* | xargs)
        
        # Filter the input BAM file to keep only the reads from the species of interest
        samtools view --threads {params.bowtie2_samtools_threads} -b {input.bam} $LIST_STRING > {output.bam}
        
        # Index the output BAM file
        samtools index {output.bam}
        
        # Write the number of species-of-interest reads to the log file
        echo "Number of species-of-interest reads:" > {log}
        samtools view --threads {params.bowtie2_samtools_threads} -c {output.bam} >> {log}
        """


##################################################################
##                          call peaks                          ##
##################################################################

# This function is used to create a dictionary for control samples
def makeCtTxDict(smp):
    # Get the control sample associated with the given sample, using the samples_table_noInputs DataFrame
    control = "results/aligned_speciesOfInterest/" + samples_table_noInputs["Control"].loc[str(smp)] + ".bam"
    
    # Check if the control variable is a string (single control sample)
    if type(control) is str:
        # If yes, return the control sample as a string
        return control
    else:
        # If not, convert the control sample(s) to a set and return
        return set(control)


# This rule calls peaks using the SICER peak caller in a Snakemake workflow
rule call_peaks_with_sicer:
    # Input files include the treatment sample and its associated control sample(s)
    input:
        treatment="results/aligned_speciesOfInterest/{sample}.bam",
        control=lambda wildcards: makeCtTxDict(wildcards.sample)
    # Output file is the resulting peak-calling file in BED format
    output:
        f"results/sicer/{{sample}}-W{str(config['sicer_windowSize'])}-G{str(config['sicer_gapSize'])}-FDR{str(config['sicer_fdr'])}-island.bed",
    # Parameters for the SICER peak caller
    params:
        sicer_genome=config["sicer_genome"],
        sicer_windowSize=config["sicer_windowSize"],
        sicer_fragmentSize=config["sicer_fragmentSize"],
        sicer_fdr=config["sicer_fdr"],
        sicer_gapSize=config["sicer_gapSize"],
    # Conda environment for SICER
    conda:
        "envs/sicer2.yml"
    # Environment modules for SICER
    envmodules:
        config["sicer2"]
    # Shell command to call peaks using the SICER peak caller with the specified parameters
    shell:
        """
        sicer -t {input.treatment} -c {input.control} -s {params.sicer_genome} -w {params.sicer_windowSize} -f {params.sicer_fragmentSize} -fdr {params.sicer_fdr} -o results/sicer/ -g {params.sicer_gapSize} -cpu 12
        """

# These rules call narrow and broad peaks using MACS2 peak caller in a Snakemake workflow

# This rule calls narrow peaks using the MACS2 peak caller
rule call_narrow_peaks_with_macs2:
    # Input files include the treatment sample and its associated control sample(s)
    input:
        treatment="results/aligned_speciesOfInterest/{sample}.bam",
        control=lambda wildcards: makeCtTxDict(wildcards.sample)
    # Output file is the resulting narrow peak-calling file in narrowPeak format
    output:
        f"results/macs2_normalPeaks/{{sample}}_{str(config['macs2_minimum_FDR_cutoff'])}_peaks.narrowPeak",
    # Parameters for the MACS2 peak caller
    params:
        effective_genome_size=config["effective_genome_size"],
        minimum_FDR_cutoff=str(config["macs2_minimum_FDR_cutoff"]),
        sample_name="{sample}",
    # Conda environment for MACS2
    conda:
        "envs/callPeaks.yml"
    # Environment modules for MACS2
    envmodules:
        config["macs2"]
    # Log file for the MACS2 narrow peak calling
    log:
        f"results/logs/snakelogs/call_narrow_peaks_with_macs2.{{sample}}_q{str(config['macs2_minimum_FDR_cutoff'])}.log",
    # Shell command to call narrow peaks using the MACS2 peak caller with the specified parameters
    shell:
        """
        macs2 --version > {log}
        macs2 callpeak -t {input.treatment} -c {input.control} -f BAMPE -g {params.effective_genome_size} -n {params.sample_name}_{params.minimum_FDR_cutoff} -q {params.minimum_FDR_cutoff} --outdir results/macs2_normalPeaks/
        """

# This rule calls broad peaks using MACS2 in a Snakemake workflow
rule call_broad_peaks_with_macs2:
    # Input is the treatment and control BAM files for species of interest
    input:
        treatment="results/aligned_speciesOfInterest/{sample}.bam",
        control=lambda wildcards: makeCtTxDict(wildcards.sample)
    # Output is the broad peak file
    output:
        f"results/macs2_broadPeaks/{{sample}}_{str(config['macs2_broad_minimum_FDR_cutoff'])}_peaks.broadPeak",
    # Parameters for the macs2 callpeak command
    params:
        effective_genome_size=config["effective_genome_size"],
        minimum_FDR_cutoff=config["macs2_broad_minimum_FDR_cutoff"],
        sample_name="{sample}",
    # Conda environment for MACS2
    conda:
        "envs/callPeaks.yml"
    # Environment modules for MACS2
    envmodules:
        config["macs2"]
    # Log file for the macs2 callpeak command
    log:
        "results/logs/snakelogs/call_broad_peaks_with_macs2.{sample}_q"
        + str(config["macs2_broad_minimum_FDR_cutoff"])
        + ".log",
    # Shell command to call broad peaks using macs2 callpeak
    shell:
        """
        # Output MACS2 version to log file
        macs2 --version > {log}
        # Call broad peaks using MACS2 with the specified parameters
        macs2 callpeak -t {input.treatment} -c {input.control} -f BAMPE -g {params.effective_genome_size} -n {params.sample_name}_{params.minimum_FDR_cutoff} -q {params.minimum_FDR_cutoff} --broad --outdir results/macs2_broadPeaks/
        """

##################################################################
##                        Make Bigwigs                          ##
##################################################################
# This rule creates BigWig files without spike-ins using the deepTools bamCoverage tool in a Snakemake workflow

rule make_bigwigs_no_spikein:
    # Input file is the aligned species of interest BAM file
    input:
        bam="results/aligned_speciesOfInterest/{sample}.bam",
    # Output file is the resulting BigWig file without spike-ins
    output:
        bw="results/bigwigs_no_spikein/{sample}.bw",
    # Parameters for the deepTools bamCoverage tool
    params:
        gs=config["effective_genome_size"],
        bs=config["bamCoverage_binSize"],
        sl=config["bamCoverage_smoothLength"],
        bl=config["blacklist_file"],
    # Conda environment for deepTools
    conda:
        "envs/deeptools.yml"
    # Environment modules for deepTools
    envmodules:
        config["deeptools"]
    # Log file for the deepTools bamCoverage tool
    log:
        "results/logs/snakelogs/make_bigwigs_no_spikein_{sample}.log",
    # Shell command to create BigWig files without spike-ins using the deepTools bamCoverage tool with the specified parameters
    shell:
        """
        bamCoverage --version > {log}
        bamCoverage -b {input.bam} -o {output.bw} --effectiveGenomeSize {params.gs} --binSize {params.bs} --smoothLength {params.sl} --numberOfProcessors max --verbose --blackListFileName {params.bl} --centerReads
        """

# This rule calculates spike-in scale factors for the input BAM files in a Snakemake workflow. 
# The rule uses samtools to count the total reads and spike-in reads in the input BAM file. It then calculates the spike-in percentage and scale factor, 
# which are written to a text file in the "results/scale_factors/" directory.
        
rule calculate_spikein_scale_factors:
    # Input is the merged BAM file
    input:
        bam="results/merged/{sample}.bam",
    # Output is a text file containing the spike-in scale factor
    output:
        report("results/scale_factors/{sample}_scaleFactor.txt"),
    # Parameter for the spike-in chromosome prefix
    params:
        px=config["spike_in_chrom_prefix"],
    # Conda environment for samtools
    conda:
        "envs/samtools.yml"
    # Environment modules for samtools
    envmodules:
        config["samtools"]
    # Shell command to calculate the spike-in scale factors and write them to the output text file
    shell:
        """
        # Count total reads in the input BAM file
        total_counts=$(samtools idxstats {input.bam} | awk -F'\t' '{{sum+=$3;}} END{{print sum;}}')
        # Count spike-in reads in the input BAM file
        spike_in_counts=$(samtools idxstats {input.bam} | grep {params.px} | awk -F'\t' '{{sum+=$3;}} END{{print sum;}}')
        # Calculate the spike-in percentage
        spike_in_percentage=$(echo "scale=3; $spike_in_counts/$total_counts*100" | bc )
        # Calculate the scale factor
        scale_factor=$(echo "scale=8; 1000000/$spike_in_counts" | bc )
        
        # Write the calculated values to a temporary output file
        echo "bam file:  {input.bam}" > {output}_tmp
        echo "total counts = ${{total_counts}}" >> {output}_tmp
        echo "spike in read counts = ${{spike_in_counts}}" >> {output}_tmp
        echo "spike in percentage = ${{spike_in_percentage}}" >> {output}_tmp
        printf "scale factor = ""%-20.8f\\n" "${{scale_factor}}" >> {output}_tmp
        
        # Move the temporary output file to the final output file
        mv {output}_tmp {output}
        """

# This rule creates BigWig files with spike-in normalization in a Snakemake workflow

rule make_bigwigs_with_spikein:
    # Input is the spike-in scale factor text file and the BAM file with species of interest reads
    input:
        scale="results/scale_factors/{sample}_scaleFactor.txt",
        bam="results/aligned_speciesOfInterest/{sample}.bam",
    # Output is the BigWig file with spike-in normalization
    output:
        bw=f"results/bigwigs_spikein/{{sample}}_{str(config['spike_in_chrom_prefix'])}.bw"
    # Parameters for the bamCoverage command
    params:
        gs=config["effective_genome_size"],
        bs=config["bamCoverage_binSize"],
        sl=config["bamCoverage_smoothLength"],
        bl=config["blacklist_file"],
        px=config["spike_in_chrom_prefix"],
    # Conda environment for deeptools
    conda:
        "envs/deeptools.yml"
    # Environment modules for deeptools
    envmodules:
        config["deeptools"]
    # Log file for the rule
    log:
        "results/logs/snakelogs/make_bigwigs_with_spikein_{sample}.log",
    # Shell command to create the BigWig file with spike-in normalization
    shell:
        """
        # Write the bamCoverage version to the log file
        bamCoverage --version > {log}
        # Extract the scale factor from the input scale factor text file
        scale_factor=$(grep scale {input.scale} | sed 's/scale factor = //g')
        # Run bamCoverage with the scale factor to create the BigWig file
        bamCoverage -b {input.bam} -o {output.bw} --effectiveGenomeSize {params.gs} --binSize {params.bs} --smoothLength {params.sl} --numberOfProcessors max --verbose --blackListFileName {params.bl} --centerReads --scaleFactor ${{scale_factor}}
        """

# This rule creates normalized BigWig files using bamcompare in a Snakemake workflow

rule make_normalized_bigwigs_with_bamcompare:
    # Input is the treatment and control merged BAM files with species of interest reads
    input:
        tx_merged_bam = "results/aligned_speciesOfInterest/{sample}.bam",
        in_merged_bam = lambda wildcards: makeCtTxDict(wildcards.sample),
    # Output is the normalized BigWig file
    output:
        "results/backgroundNormalizedBigwigs/{sample}_bkgrndNorm.bw",
    # Parameters for the bamCompare command
    params:
        sfm=config["scaleFactorsMethod"],
        nmu=config["normalizeUsing"],
        bco=config["bamcompareOperation"],
        bns=config["binSize"],
        egs=config["effective_genome_size"],
        nop=config["numberOfProcessors"],
        mfl=config["minFragmentLength"],
        mxl=config["maxFragmentLength"],
        mmq=config["minMappingQuality"],
        sml=config["smoothLength"],
        ign=config["ignoreForNormalization"],
        bln=config["blacklist_file"]
    # Environment modules for deeptools
    envmodules:
        config["deeptools"],
    # Shell command to create the normalized BigWig file using bamCompare
    shell:
        """
        # Run bamCompare with the specified parameters to create the normalized BigWig file
        bamCompare --smoothLength {params.sml} --minFragmentLength {params.mfl} --ignoreForNormalization {params.ign} --maxFragmentLength {params.mxl} --minMappingQuality {params.mmq} --numberOfProcessors {params.nop} --effectiveGenomeSize {params.egs} --scaleFactorsMethod {params.sfm} --normalizeUsing {params.nmu} --operation {params.bco} --binSize {params.bns} --blackListFileName {params.bln} --centerReads --numberOfProcessors {params.nop} -b1 {input.tx_merged_bam} -b2 {input.in_merged_bam} -o {output}
        """

##################################################################
##                              QC                              ##
##################################################################
rule make_fragment_sizes_plots:
    input:
        treatment="results/aligned_speciesOfInterest/{sample}.bam",
        control=lambda wildcards: makeCtTxDict(wildcards.sample),
    output:
        fp=report(f"results/fragment_sizes_plots/{{sample}}_fragment_sizes.png"),
        fl=f"results/fragment_sizes/{{sample}}_fragment_sizes.txt",
        fs=f"results/fragment_sizes/{{sample}}_fragment_sizes_stats.txt",
    params:
        bl=config["blacklist_file"],
        processors=config["bowtie2_samtools_threads"],
        max_size_to_plot=1000
    conda:
        "envs/deeptools.yml"
    envmodules:
        config["deeptools"]
    shell:
        """
        mkdir -p results/fragment_sizes_plots/
        mkdir -p results/fragment_sizes/
        touch {output.fl}
        bamPEFragmentSize --maxFragmentLength {params.max_size_to_plot} --outRawFragmentLengths {output.fl} --table {output.fs} --numberOfProcessors {params.processors} --blackListFileName {params.bl} -b {input.treatment} {input.control} --histogram {output.fp}
        """


rule make_fingerprint_plots:
    input:
        treatment="results/aligned_speciesOfInterest/{sample}.bam",
        control=lambda wildcards: makeCtTxDict(wildcards.sample),
    output:
        fp=report(f"results/fingerprint_plots/{{sample}}_fingerprint.png"),
        qm=f"results/fingerprint_quality_metrics/{{sample}}_fingerprint_quality_metrics.txt",
    params:
        bl=config["blacklist_file"],
    conda:
        "envs/deeptools.yml"
    envmodules:
        config["deeptools"]
    shell:
        """
        plotFingerprint --outQualityMetrics {output.qm} --JSDsample {input.control} --numberOfProcessors max --smartLabels --blackListFileName {params.bl} -b {input.treatment} {input.control} -plot {output.fp}
        """


rule calculate_alignment_and_filtering_counts:
    input:
        filtered_bam="results/merged/{sample}.bam",
        species_bam="results/aligned_speciesOfInterest/{sample}.bam",
    output:
        "results/read_stats/{sample}.csv"
    conda:
        "envs/samtools.yml"
    envmodules:
        config["samtools"]
    log:
    shell:
        """
        FILTERED_READS=$(samtools view -c {input.filtered_bam})
        SPECIES_READS=$(samtools view -c {input.species_bam})
        echo "filtered read count, ${{FILTERED_READS}}" >> {output}_tmp
        echo "goi read count, ${{SPECIES_READS}}" >> {output}_tmp
        mv {output}_tmp {output}
        """


rule make_html_report:
    input:
        rs=expand("results/read_stats/{sample}.csv",sample=all_samples),
        fp=expand(f"results/fingerprint_plots/{{sample}}_fingerprint.png",sample=merged_samples_noInputs),
        fm=expand(f"results/fingerprint_quality_metrics/{{sample}}_fingerprint_quality_metrics.txt",sample=merged_samples_noInputs),
        fs=expand(f"results/fragment_sizes_plots/{{sample}}_fragment_sizes.png",sample=merged_samples_noInputs),
    output:
        "results/Report.html"
    params:
        samples=merged_samples_noInputs,
    conda:
        "envs/R_Report.yml"
    envmodules:
        config["R"],
        config["Bioconductor"]
    log: "results/logs/snakelogs/make_html_report.log"
    script:
        "scripts/Make_final_report.R"   

rule dummy:
    output:
        ".placeholder"
    shell:
        """
        touch {output}
        """
